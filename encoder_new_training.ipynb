{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99c8932-83d7-4b04-bef4-3544cd40aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "from encoder_layer import Encoder_block\n",
    "from positional_encoding import Positional_Encoding\n",
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05197d38-d479-4a1b-a05c-dc4691b0b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"kp20k_local\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca74dec0-bb44-4ab0-8d89-64e42f918648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'doc_bio_tags', 'extractive_keyphrases', 'abstractive_keyphrases', 'other_metadata'],\n",
       "        num_rows: 530809\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'doc_bio_tags', 'extractive_keyphrases', 'abstractive_keyphrases', 'other_metadata'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'document', 'doc_bio_tags', 'extractive_keyphrases', 'abstractive_keyphrases', 'other_metadata'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e002e24e-f1dd-480f-9d0a-21b7ae6e5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    texts = [\" \".join(tokens) for tokens in batch[\"document\"]]\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=False,          # padding baad mein collator karega\n",
    "        max_length=256,\n",
    "        return_offsets_mapping=False\n",
    "    )\n",
    "    tokenized[\"word_ids\"] = [tokenized.word_ids(i) for i in range(len(texts))]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240fba62-b93c-4009-9583-951c874655ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_batch, batched=True, remove_columns=[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d939ed0-cf46-42f8-86f3-022c128a2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {\"O\": 0, \"B\": 1, \"I\": 2}\n",
    "\n",
    "def align_labels(batch):\n",
    "    labels = []\n",
    "    for tags, word_ids in zip(batch[\"doc_bio_tags\"], batch[\"word_ids\"]):\n",
    "        tags = [tag2id[t] for t in tags]\n",
    "        new_labels = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                new_labels.append(-100)\n",
    "            elif wid != prev_wid and wid < len(tags):\n",
    "                new_labels.append(tags[wid] if tags[wid] == 0 else 2)  # B ko I bana do subwords ke liye\n",
    "            elif wid < len(tags) and tags[wid] in [1,2]:\n",
    "                new_labels.append(2)\n",
    "            else:\n",
    "                new_labels.append(-100)\n",
    "            prev_wid = wid\n",
    "        labels.append(new_labels)\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d6cc25-1766-49e6-8500-295ad16c3172",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"]\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [col for col in tokenized_dataset[\"train\"].column_names if col not in final_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e522356c-6aca-454c-bb69-2ebb626e5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63545149-221b-44b9-9fe3-3dd0a44c0349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertModel\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert_emb = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").get_input_embeddings()\n",
    "bert_emb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7d7404-2a4c-4e97-bab8-e60ef3e03e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = nn.Linear(768, 512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad111bbe-6bac-4176-b419-3bc68fc74460",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    nn.init.xavier_uniform_(projection.weight)\n",
    "    projection.bias.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cbc6d5a-87de-4696-8d15-cbbb8d96a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7faf96c7-918f-4250-934a-fd7e9cc09b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"x:\", x.shape)\n",
    "# print(\"attention_mask:\", attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ffe385-96ff-4375-b4bd-09f87f2009c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder_block(d_model=d_model, num_heads=num_heads, d_ff=d_ff).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c89a5db0-720e-4598-898b-aa1fb204d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Linear(d_model, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd637b4-e3ee-4a94-b19f-8ace32a60bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emb.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f1c1d6-6697-4299-bcf3-9668e5f0f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-4)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(projection.parameters()) + list(encoder.parameters()) + list(classifier.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor([0.05, 1.0, 1.0]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a156a146-dba5-4682-a70a-de338df1dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_collate(batch):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in batch]\n",
    "    attention_mask = [torch.tensor(item[\"attention_mask\"]) for item in batch]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in batch]\n",
    "\n",
    "    # Pad kar do\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 ignore hota hai loss mein\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eb9a141-5ef8-43b9-8eb4-70cf07357b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=smart_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fe9d78c-e0a2-4523-b632-72bca2082788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|â–                                                      | 67/16588 [00:07<30:57,  8.89it/s, loss=0.592]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     51\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 53\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mtotal_loss \u001b[38;5;241m/\u001b[39m (loop\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     56\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a logs directory if it doesn't exist\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='logs/training.log', \n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "encoder.train()\n",
    "classifier.train()\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Sirf embedding part BERT se\n",
    "        with torch.no_grad():\n",
    "            x = bert_emb(input_ids)  # (B, L, 768)\n",
    "\n",
    "        x = projection(x)        \n",
    "\n",
    "        # print(\"x.shape:\", x.shape)  # should be (batch_size, seq_len, embed_dim)\n",
    "        # print(\"attention_mask.shape:\", attention_mask.shape)  # should be (batch_size, seq_len)\n",
    "\n",
    "        # mask = attention_mask.unsqueeze(1).unsqueeze(2)   # (B, 1, 1, seq_len)\n",
    "        encoder_out, attn_weights = encoder(x, mask=attention_mask)\n",
    "\n",
    "        # Yaha se tera apna encoder shuru\n",
    "        # encoder_out, attn_weights = encoder(x, mask=attention_mask)  # tera function mask support karta hoga na?\n",
    "        logits = classifier(encoder_out)\n",
    "\n",
    "        loss = criterion(logits.view(-1, 3), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=total_loss / (loop.n + 1))\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    logging.info(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    loop.write(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': encoder.state_dict(),\n",
    "        'classifier_state_dict': classifier.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    logging.info(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b4e42-06b7-49b7-9e99-c478e5cbd51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

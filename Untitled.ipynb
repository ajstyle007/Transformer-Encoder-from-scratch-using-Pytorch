{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9c3c4da-629b-4a80-959a-ea7bb8d6cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder_layer import Encoder_block\n",
    "from positional_encoding import Positional_Encoding\n",
    "from transformers import BertTokenizerFast, DistilBertModel, DistilBertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- TOKENIZER (MUST BE SAME AS TRAINING) ---\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    model_max_length=256\n",
    ")\n",
    "\n",
    "# --- DISTILBERT EMBEDDINGS ---\n",
    "bert_emb = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").get_input_embeddings()\n",
    "\n",
    "# --- PROJECTION (YOU USED THIS IN TRAINING) ---\n",
    "projection = nn.Linear(768, 512)\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "\n",
    "encoder = Encoder_block(d_model=d_model, num_heads=num_heads, d_ff=d_ff)\n",
    "classifier = nn.Linear(d_model, 3)\n",
    "\n",
    "# --- LOAD CHECKPOINT ---\n",
    "ckpt = torch.load(\"restored_model.pt\", map_location=\"cpu\")\n",
    "encoder.load_state_dict(ckpt[\"encoder_state_dict\"])\n",
    "classifier.load_state_dict(ckpt[\"classifier_state_dict\"])\n",
    "projection.load_state_dict(ckpt[\"projection_state_dict\"])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "encoder.to(device)\n",
    "classifier.to(device)\n",
    "projection.to(device)\n",
    "bert_emb.to(device)\n",
    "\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "def extract_keyphrases(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = bert_emb(input_ids)\n",
    "        x = projection(x)\n",
    "        encoder_out, attn_weights = encoder(x, mask=attention_mask)\n",
    "        logits = classifier(encoder_out)\n",
    "        preds = torch.argmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    phrases = []\n",
    "    current = []\n",
    "\n",
    "    for tok, tag in zip(tokens, preds):\n",
    "        tok = tok.replace(\"##\", \"\")\n",
    "        if tok in [\"[CLS]\",\"[SEP]\",\"[PAD]\"]:\n",
    "            continue\n",
    "\n",
    "        if tag == 1:  # B\n",
    "            if current:\n",
    "                phrases.append(\" \".join(current))\n",
    "            current = [tok]\n",
    "        elif tag == 2:  # I\n",
    "            if current:\n",
    "                current.append(tok)\n",
    "        else:\n",
    "            if current:\n",
    "                phrases.append(\" \".join(current))\n",
    "                current = []\n",
    "\n",
    "    if current:\n",
    "        phrases.append(\" \".join(current))\n",
    "\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17335248-6a85-4705-87f4-36c07c038393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning models', 'are', 'widely', 'used', 'for prediction', 'tasks', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Machine learning models are widely used for prediction tasks.\"\n",
    "print(extract_keyphrases(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4251c7b1-00f7-4637-8d03-99bb437c4af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformers', 'use', 'attention', 'to', 'efficiently', '.']\n"
     ]
    }
   ],
   "source": [
    "phrases = extract_keyphrases(\"Transformers use attention to process sequences efficiently.\")\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e463ffe5-233a-4b7d-a599-01f620fe795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks are powerful models. => ['neural', 'networks', 'are', '.']\n",
      "Self-attention changed NLP forever. => ['attention changed', 'p forever', '.']\n",
      "Transformers replaced RNN-based architectures. => ['transformers', 'rn n', 'based', 'architecture', '.']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Neural networks are powerful models.\",\n",
    "    \"Self-attention changed NLP forever.\",\n",
    "    \"Transformers replaced RNN-based architectures.\"\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    print(t, \"=>\", extract_keyphrases(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08d5aad6-b4eb-4313-be9a-b317b92d545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_token_predictions(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = bert_emb(input_ids)\n",
    "        x = projection(x)\n",
    "        encoder_out, _ = encoder(x, mask=attention_mask)\n",
    "        logits = classifier(encoder_out)\n",
    "        preds = torch.argmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    print(\"\\n--- DEBUG TOKEN PREDICTIONS ---\")\n",
    "    for tok, pid, pred in zip(tokens, input_ids[0].tolist(), preds):\n",
    "        print(f\"{tok:15}  ID={pid:5}  →  Pred={pred}\")\n",
    "    print(\"--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f8954bc-70d8-48e2-919d-7f8646faa7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUG TOKEN PREDICTIONS ---\n",
      "[CLS]            ID=  101  →  Pred=0\n",
      "neural           ID=15756  →  Pred=1\n",
      "networks         ID= 6125  →  Pred=1\n",
      "are              ID= 2024  →  Pred=1\n",
      "powerful         ID= 3928  →  Pred=0\n",
      "models           ID= 4275  →  Pred=2\n",
      ".                ID= 1012  →  Pred=1\n",
      "[SEP]            ID=  102  →  Pred=1\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debug_token_predictions(\"Neural networks are powerful models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c699aefd-d18b-4839-8b04-c3bf0c92d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag = {0: \"O\", 1: \"B\", 2: \"I\"}\n",
    "\n",
    "def predict_sentence(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", is_split_into_words=False)\n",
    "\n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = bert_emb(input_ids)\n",
    "        x = projection(x)\n",
    "        enc_out, _ = encoder(x, mask=attention_mask)\n",
    "        logits = classifier(enc_out)\n",
    "        preds = torch.argmax(logits, dim=-1)[0].tolist()\n",
    "\n",
    "    tokens_decoded = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "    clean_output = []\n",
    "    for tok, pred in zip(tokens_decoded, preds):\n",
    "        if tok in [\"[CLS]\", \"[SEP]\"]:\n",
    "            continue\n",
    "        clean_output.append((tok, pred, id2tag[pred]))\n",
    "\n",
    "    return clean_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dcb84f3-5829-4f1c-b24e-21d1d553010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1, 'B'), ('double', 1, 'B'), ('helix', 2, 'I'), ('structure', 1, 'B'), ('of', 0, 'O'), ('dna', 1, 'B')]\n"
     ]
    }
   ],
   "source": [
    "test_tokens = [\"the\", \"double\", \"helix\", \"structure\", \"of\", \"dna\"]\n",
    "print(predict_sentence(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a78d9126-4e43-4e16-9b08-83d70ac8c630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1, 'B'),\n",
       " ('double', 1, 'B'),\n",
       " ('helix', 2, 'I'),\n",
       " ('structure', 1, 'B'),\n",
       " ('of', 0, 'O'),\n",
       " ('dna', 1, 'B'),\n",
       " ('was', 1, 'B'),\n",
       " ('published', 0, 'O'),\n",
       " ('in', 0, 'O'),\n",
       " ('1953', 1, 'B')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The double helix structure of DNA was published in 1953\"\n",
    "tokens = sentence.lower().split()\n",
    "predict_sentence(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e434217-2d77-4550-aaab-ad9d39c31f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels(batch):\n",
    "    new_batch_labels = []\n",
    "\n",
    "    for tags, word_ids in zip(batch[\"doc_bio_tags\"], batch[\"word_ids\"]):\n",
    "        tags = [tag2id[t] for t in tags]\n",
    "        aligned = []\n",
    "\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned.append(-100)\n",
    "            else:\n",
    "                if wid != prev_wid:\n",
    "                    aligned.append(tags[wid])   # use true tag\n",
    "                else:\n",
    "                    if tags[wid] == 0:\n",
    "                        aligned.append(0)\n",
    "                    else:\n",
    "                        aligned.append(2)\n",
    "            prev_wid = wid\n",
    "        new_batch_labels.append(aligned)\n",
    "\n",
    "    batch[\"labels\"] = new_batch_labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7f1c804-dbb2-4c9e-a9d6-97f3fbd59a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from encoder_layer import Encoder_block\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- 1) Correct Tokenizer ---\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# --- 2) Correct Embedding ---\n",
    "bert_emb = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").get_input_embeddings().to(device)\n",
    "\n",
    "# --- 3) Load your modules ---\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "\n",
    "projection = nn.Linear(768, 512)\n",
    "encoder = Encoder_block(d_model=d_model, num_heads=num_heads, d_ff=d_ff)\n",
    "classifier = nn.Linear(512, 3)\n",
    "\n",
    "ckpt = torch.load(\"restored_model.pt\", map_location=device)\n",
    "\n",
    "projection.load_state_dict(ckpt[\"projection_state_dict\"])\n",
    "encoder.load_state_dict(ckpt[\"encoder_state_dict\"])\n",
    "classifier.load_state_dict(ckpt[\"classifier_state_dict\"])\n",
    "\n",
    "projection.to(device)\n",
    "encoder.to(device)\n",
    "classifier.to(device)\n",
    "\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "STOPWORDS = {\"the\", \"a\", \"an\", \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"to\", \"in\", \"of\"}\n",
    "\n",
    "def clean_phrase(p):\n",
    "    words = p.split()\n",
    "    words = [w for w in words if w.lower() not in STOPWORDS]\n",
    "    return \" \".join(words).strip()\n",
    "\n",
    "def extract_keyphrases(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = bert_emb(input_ids)\n",
    "        x = projection(x)\n",
    "        enc_out, attn = encoder(x, mask=attention_mask)\n",
    "        logits = classifier(enc_out)\n",
    "        preds = torch.argmax(logits, dim=-1)[0].cpu().tolist()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    phrases = []\n",
    "    curr = []\n",
    "\n",
    "    for tok, p in zip(tokens, preds):\n",
    "        if tok in [\"[CLS]\", \"[SEP]\"]:\n",
    "            continue\n",
    "\n",
    "        tok = tok.replace(\"##\", \"\")\n",
    "\n",
    "        if p == 1:  # B\n",
    "            if curr:\n",
    "                phrases.append(clean_phrase(\" \".join(curr)))\n",
    "            curr = [tok]\n",
    "\n",
    "        elif p == 2:  # I\n",
    "            curr.append(tok)\n",
    "\n",
    "        else:  # O\n",
    "            if curr:\n",
    "                phrases.append(clean_phrase(\" \".join(curr)))\n",
    "                curr = []\n",
    "\n",
    "    if curr:\n",
    "        phrases.append(clean_phrase(\" \".join(curr)))\n",
    "\n",
    "    # remove empty\n",
    "    phrases = [p for p in phrases if p]\n",
    "\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5ee96c9-4fcb-4977-b04f-62957fb668ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['double helix', 'structure', 'dna', '1953']\n"
     ]
    }
   ],
   "source": [
    "print(extract_keyphrases(\"The double helix structure of DNA was published in 1953\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3597286-bfcd-41ff-8db9-633ab6ce4c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intelligence', 'transforming modern', 'healthcare through', 'advanced', 'models', '.']\n"
     ]
    }
   ],
   "source": [
    "print(extract_keyphrases(\"Artificial intelligence is transforming modern healthcare through advanced innovative models.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06e3e1ab-6f88-47e7-9845-1438572debac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks are powerful models. => ['neural', 'networks', 'models', '.']\n",
      "Self-attention changed NLP forever. => ['attention changed', 'p forever', '.']\n",
      "Transformers replaced RNN-based architectures. => ['transformers', 'rn n', 'based', 'architecture', '.']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Neural networks are powerful models.\",\n",
    "    \"Self-attention changed NLP forever.\",\n",
    "    \"Transformers replaced RNN-based architectures.\"\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    print(t, \"=>\", extract_keyphrases(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36bfe4cc-f65c-4f45-bac1-acab4735eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba5da88b-b70e-41dd-b476-335f88462373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = bert_emb(input_ids)\n",
    "        x = projection(x)\n",
    "        enc_out, _ = encoder(x, mask=attention_mask)\n",
    "\n",
    "        # Mean pooling (mask-aware)\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(enc_out.size())\n",
    "        sum_emb = torch.sum(enc_out * mask_expanded, dim=1)\n",
    "        length = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        sentence_embedding = sum_emb / length\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(512*3, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1),\n",
    "    nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "mlp.eval()\n",
    "\n",
    "def similarity(sent1, sent2):\n",
    "    e1 = get_sentence_embedding(s1)\n",
    "    e2 = get_sentence_embedding(s2)\n",
    "    \n",
    "    cos = torch.nn.functional.cosine_similarity(e1, e2)\n",
    "    \n",
    "    abs_diff = torch.abs(e1 - e2)\n",
    "    \n",
    "    mlp_input = torch.cat([e1, e2, abs_diff], dim=-1)\n",
    "    score = mlp(mlp_input)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ded36f5-e857-46d2-9fe3-495f54014012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: tensor([[0.5044]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s1 = \"Artificial intelligence is transforming the world.\"\n",
    "s2 = \"AI is changing how we live.\"\n",
    "\n",
    "score = similarity(s1, s2)\n",
    "print(\"Similarity Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "251c6f04-55f6-4e58-86e7-575b38a9528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: tensor([[0.4882]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s1 = \"The moon orbits around the Earth and reflects sunlight.\"\n",
    "s2 = \"Fresh vegetables should be stored in a cool and dry place.\"\n",
    "\n",
    "score = similarity(s1, s2)\n",
    "print(\"Similarity Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30be253a-3811-485c-b486-ce398dfdad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: tensor([[0.5065]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "Similarity Score: tensor([[0.4914]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s1 = \"Deep learning is revolutionizing computer vision.\"\n",
    "s2 = \"Neural networks are transforming the field of computer vision.\"\n",
    "\n",
    "print(\"Similarity Score:\", similarity(s1, s2))\n",
    "\n",
    "s1 = \"The cat slept peacefully on the sofa.\"\n",
    "s2 = \"Quantum mechanics describes the behavior of particles.\"\n",
    "\n",
    "print(\"Similarity Score:\", similarity(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f044dd44-3f0d-43bd-9f4a-3e9dcafbae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: tensor([[0.4914]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "Similarity Score: tensor([[0.4909]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a1= \"Climate change is affecting global weather patterns.\"\n",
    "a2= \"Environmental changes are impacting weather across the world.\"\n",
    "\n",
    "print(\"Similarity Score:\", similarity(a1, a2))\n",
    "\n",
    "s1= \"I am planning to travel to Japan next month.\"\n",
    "s2= \"The bakery sells fresh bread every morning.\"\n",
    "\n",
    "print(\"Similarity Score:\", similarity(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd28a012-d463-432d-a0b3-7a94395750ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: tensor([[0.4909]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "Similarity Score: tensor([[0.4933]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a1= \"He bought a new smartphone yesterday.\"\n",
    "a2= \"Yesterday, he purchased a brand-new phone.\"\n",
    "\n",
    "print(\"Similarity Score:\", similarity(a1, a2))\n",
    "\n",
    "s1= \"The ocean waves were calm and soothing.\"\n",
    "s2= \"He solved a complex algebraic equation in seconds.\"\n",
    "\n",
    "print(\"Similarity Score:\", similarity(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5cb0d1e7-8ce7-4a70-a565-c2ddd8aae12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9991123676300049\n"
     ]
    }
   ],
   "source": [
    "def emb(sent):\n",
    "    inputs = tokenizer(sent, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        x = bert_emb(inputs[\"input_ids\"])\n",
    "        x = projection(x)\n",
    "        enc_out, _ = encoder(x, mask=inputs[\"attention_mask\"])\n",
    "        # sent_emb = enc_out.mean(dim=1)\n",
    "        sent_emb = enc_out[:, 0, :] \n",
    "    return sent_emb\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return torch.nn.functional.cosine_similarity(a, b).item()\n",
    "\n",
    "print(cosine_sim(emb(s1), emb(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4da98d0-d913-45a4-b311-7bc1cba5f430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988912343978882\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim(emb(\"The cat is sleeping.\"), emb(\"The stock market crashed yesterday.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647d071-cc7c-40de-84a2-4c2ed647249b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
